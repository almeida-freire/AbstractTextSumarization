{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "    \n",
    "    def __init__(\n",
    "        self, *, \n",
    "        #PKL ARGS\n",
    "        pkl_text_path=\"\", pkl_summary_path=\"\", pkl_word2idx_path=\"\", pkl_idx2word_path=\"\",\n",
    "        #BUILD PKL ARGS\n",
    "        text_path=\"\", summary_path=\"\", max_chr_count=300, dataset_size=-1,\n",
    "        #SAVE\n",
    "        save_path=\"\",\n",
    "        #FLAGS\n",
    "        build_from_dataset=False\n",
    "    ):\n",
    "        if pkl_text_path != \"\" and pkl_summary_path != \"\" and pkl_word2idx_path != \"\" and pkl_idx2word_path != \"\" and \\\n",
    "                os.path.isfile(pkl_text_path) and os.path.isfile(pkl_summary_path) and os.path.isfile(pkl_word2idx_path) and \\\n",
    "                os.path.isfile(pkl_idx2word_path) and not build_from_dataset:\n",
    "            self._load_pkl(pkl_text_path, pkl_summary_path, pkl_word2idx_path, pkl_idx2word_path)\n",
    "                \n",
    "        elif text_path !=\"\" and summary_path != \"\":\n",
    "            self.force_load_from_files(text_path, summary_path, max_chr_count, dataset_size)\n",
    "            self.override_files_with_current_data(save_path)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Wrong argument combination provided.\")\n",
    "            \n",
    "    def override_files_with_current_data(self, save_path):\n",
    "        if save_path != \"\":\n",
    "            print(\"Overriding files with current data.\")\n",
    "            \n",
    "            with open(save_path+\"/dataset_text.pkl\", \"wb\") as data_file:\n",
    "                pickle.dump(self.dataset_text, data_file)\n",
    "            with open(save_path+\"/dataset_summary.pkl\", \"wb\") as data_file:\n",
    "                pickle.dump(self.dataset_summary, data_file)\n",
    "            with open(save_path+\"/word2idx.pkl\", \"wb\") as data_file:\n",
    "                pickle.dump(self.word2idx, data_file)\n",
    "            with open(save_path+\"/idx2word.pkl\", \"wb\") as data_file:\n",
    "                pickle.dump(self.idx2word, data_file)\n",
    "\n",
    "            print(\"Data saved to files.\")\n",
    "\n",
    "    def force_load_from_files(self, text_path, summary_path, max_chr_count, dataset_size):\n",
    "        print(\"Building data from dataset.\")\n",
    "        \n",
    "        self.dataset_text, self.dataset_summary = self._load_dataset(\n",
    "            text_path=text_path, summary_path=summary_path, \n",
    "            max_chr_count=max_chr_count, dataset_size=dataset_size\n",
    "        )\n",
    "        self.word2idx, self.idx2word = self._create_vocabulary()\n",
    "            \n",
    "    def _load_pkl(self, pkl_text_path, pkl_summary_path, pkl_word2idx_path, pkl_idx2word_path):\n",
    "        with open(pkl_text_path, \"rb\") as data_file:\n",
    "            self.dataset_text = pickle.load(data_file)\n",
    "        with open(pkl_summary_path, \"rb\") as data_file:\n",
    "            self.dataset_summary = pickle.load(data_file)\n",
    "        with open(pkl_word2idx_path, \"rb\") as data_file:\n",
    "            self.word2idx = pickle.load(data_file)\n",
    "        with open(pkl_idx2word_path, \"rb\") as data_file:\n",
    "            self.idx2word = pickle.load(data_file)\n",
    "\n",
    "        print(\"PKL dataset loaded with vocab size ({}) and dataset len ({})\".format(\n",
    "            len(self.idx2word), len(self.dataset_text)\n",
    "        ))\n",
    "\n",
    "    def _load_dataset(self, text_path, summary_path, max_chr_count=300, dataset_size=-1):\n",
    "        with open(text_path, \"r\") as text_file:\n",
    "            text = text_file.read().lower().strip().split(\"\\n\")\n",
    "        with open(summary_path, \"r\") as summary_file:\n",
    "            summary = summary_file.read().lower().strip().split(\"\\n\")\n",
    "\n",
    "        if len(text) != len(summary):\n",
    "            raise RuntimeError(\"Dataset Inconsistency.\")\n",
    "\n",
    "        dataset_text = []\n",
    "        dataset_summary = []\n",
    "\n",
    "        size = min(len(text), dataset_size) if dataset_size != -1 else len(text)\n",
    "\n",
    "        for line_idx in range(size):\n",
    "            print(\"\\r <{progress}{left}>\".format(\n",
    "                progress=\"=\"*int((line_idx/size)*30),\n",
    "                left=\" \"*int(((size-line_idx)/size)*30)\n",
    "            ), end=\"\")\n",
    "\n",
    "            token_texts = word_tokenize(text[line_idx])\n",
    "            token_summaries = word_tokenize(summary[line_idx])\n",
    "            \n",
    "            if len(token_texts) <= max_chr_count and len(token_summaries) <= max_chr_count:\n",
    "                dataset_text.append(token_texts)\n",
    "                dataset_summary.append(token_summaries)\n",
    "        \n",
    "        print(\"\")                \n",
    "                \n",
    "        return dataset_text, dataset_summary\n",
    "    \n",
    "    def _create_vocabulary(self):\n",
    "        vocabulary = set()\n",
    "        \n",
    "        for idx, (words_text, words_summary) in enumerate(zip(self.dataset_text, (self.dataset_summary))):\n",
    "            for word in words_text:\n",
    "                vocabulary.update([word])\n",
    "\n",
    "            for word in words_summary:\n",
    "                vocabulary.update([word])\n",
    "                \n",
    "        vocabulary.update([\"<SOS>\"])\n",
    "        vocabulary.update([\"<EOS>\"])\n",
    "        vocabulary.update([\"<UNK>\"])\n",
    "               \n",
    "        vocabulary = list(vocabulary)\n",
    "        vocabulary.sort()\n",
    "            \n",
    "        return {word: idx for idx, word in enumerate(vocabulary)}, vocabulary\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        rounds = math.ceil(len(self.dataset_text)/batch_size)\n",
    "\n",
    "        for batch_num in range(rounds):\n",
    "            sample_text = self.dataset_text[batch_num*batch_size : (batch_num+1)*batch_size]\n",
    "            sample_summary = self.dataset_summary[batch_num*batch_size : (batch_num+1)*batch_size]\n",
    "            \n",
    "            sample_text = [[self.word2idx[word] for word in sentence] for sentence in sample_text]\n",
    "            sample_summary = [[self.word2idx[word] for word in sentence] for sentence in sample_summary]\n",
    "            \n",
    "            sample_text_sizes = [len(sentence) for sentence in sample_text]\n",
    "            sample_summary_sizes = [len(sentence) for sentence in sample_summary]\n",
    "            \n",
    "            sample_text_max_len = max(sample_text_sizes) + 2\n",
    "            sample_summary_max_len = max(sample_summary_sizes) + 2\n",
    "            \n",
    "            sample_text = [[self.word2idx[\"<SOS>\"]] + sentence + [self.word2idx[\"<EOS>\"]]*(sample_text_max_len - 1 - len(sentence)) for sentence in sample_text]\n",
    "            sample_summary = [[self.word2idx[\"<SOS>\"]] + sentence + [self.word2idx[\"<EOS>\"]]*(sample_summary_max_len - 1 - len(sentence)) for sentence in sample_summary]\n",
    "            \n",
    "            yield (sample_text, sample_summary), (sample_text_sizes, sample_summary_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building data from dataset.\n",
      " <===================          >"
     ]
    }
   ],
   "source": [
    "h = Helper(\n",
    "    #PKL ARGS\n",
    "    pkl_text_path=\"./pkl_dataset/dataset_text.pkl\", pkl_summary_path=\"./pkl_dataset/dataset_summary.pkl\", \n",
    "    pkl_word2idx_path=\"./pkl_dataset/word2idx.pkl\", pkl_idx2word_path=\"./pkl_dataset/idx2word.pkl\",\n",
    "    #BUILD PKL ARGS\n",
    "    text_path=\"../Dataset/train/train.article.txt\", summary_path=\"../Dataset/train/train.title.txt\", max_chr_count=300, dataset_size=-1,\n",
    "    #SAVE\n",
    "    save_path=\"./pkl_dataset\",\n",
    "    #FLAGS\n",
    "    build_from_dataset=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
